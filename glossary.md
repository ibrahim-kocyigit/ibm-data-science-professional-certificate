## Glossary

**ACID-compliance**: Ensuring data accuracy and consistency through Atomicity, Consistency, Isolation, and Durability in database transactions.

**Adobe Spark:** A suite of software tools that allow users to create and share visual content such as graphics, web pages, and videos.

**Algorithm:** A set of step-by-step instructions to solve a problem or complete a task.

**Analytic Approach:** The process of selecting the appropriate method or path to address a specific data science question or problem.

**Analytical Skills:** The ability to analyze information systematically, logically, and organized.

**Analytics:** The systematic analysis of data using statistical, mathematical, and computational techniques to uncover insights, patterns, and trends.

**Analytics Team:** A group of professionals, including data scientists and analysts, responsible for performing data analysis and modeling. 

**Apache MLlib:** Language that makes machine learning scalable.

**Apache Spark:** A general-purpose cluster-computing framework allowing you to process data using compute clusters.

**API:** Application Programming Interface allows communication between two pieces of software.

**Arithmetic Models:** Data science often uses mathematical models to analyze data and predict outcomes.

**Artificial Neural Networks:** Collections of small computing units (neurons) that process data and learn to make decisions over time.

**Automation:** Using tools and techniques to streamline data collection and preparation processes.

**Bayesian Analysis:** A statistical technique that uses Bayes' theorem to update probabilities based on new evidence.

**Big Data:** Vast amounts of structured, semi-structured, and unstructured data. Characterized by its volume, velocity, variety, and value, which, when analyzed, can provide competitive advantages and drive digital transformations.

**Big Data Cluster:** A distributed computing environment comprising thousands or tens of thousands of interconnected computers that collectively store and process large datasets.

**Broad Network Access:** The ability to access cloud resources via standard mechanisms and platforms such as mobile devices, laptops, and workstations over networks.

**Business Understanding:** The initial phase of data science methodology involves seeking clarification and understanding the goals, objectives, and requirements of a given task or problem.

**Business Insights:** Accurate insights and reports generated by generative AI. Can be updated as data evolves, enhancing decision-making and uncovering hidden patterns.

**Caffe:** A deep learning algorithm repository built with C++, with Python and Matlab bindings.

**Case study:** In-depth analysis of an instance of a chosen subject to draw insights that inform theory, practice or decision-making.

**CDLA:** Community Data License Agreement

**Chief Data Officer (CDO):** An emerging role responsible for overseeing data-related initiatives, governance, and strategies, ensuring that data plays a central role in digital transformation efforts.

**Chief Information Officer (CIO):** An executive who is responsible for managing an organization's information technology and computer systems, contributing to technology-related aspects of digital transformation.

**Classification models:** Are used to predict whether some information or data belongs to a category or class.

**CLI:** Command line interface

**Cloud-based Integration Platform as a Service (iPaaS):** Cloud-hosted integration platforms that offer integration services through virtual private clouds or hybrid cloud models, providing scalability and flexibility.

**Cloud Computing:** The delivery of on-demand computing resources, including networks, servers, storage, applications, services, and data centers, over the internet on a pay-for-use basis.

**Cloud Deployment Models:** Categories that indicate where cloud infrastructure resides, who manages it, and how cloud resources and services are made available to users, including public, private, and hybrid models.

**Cloud Service Models:** Models based on the layers of a computing stack, including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS), represent different cloud computing offerings.

**Cluster Analysis:** The process of grouping similar data points together based on certain futures or attributes.

**Clustering Association:** An approach used to learn about human behavior and identify patterns and associations in data.

**Coding Automation:** Using generative AI to automatically generate and test software code for constructing analytical models, freeing data scientists to focus on higher-level tasks.

**Cohort:** A group of individuals who share a common characteristics or experience, is studied or analyzed as a unit.

**Cohort Study:** An observational study where a group of individuals with a specific characteristic or exposure is followed over time to determine the incidence of outcomes or the relationship between exposures and outcomes.

**Column-based Database:** A type of NoSQL database that organizes data in cells grouped as columns, often used for systems requiring high write request volume and storage of time-series or IoT data.

**Comma-Separated Values (CSV) / Tab-Separated Values (TSv):** Commonly used format for storing tabular data as plain text where either the comma or the tab separates each value.

**Commodity Hardware:** Standard, off-the-shelf hardware components, are used in a big data cluster, offering cost-effective solutions for storage and processing without relying on specialized hardware.

**Computational Thinking:** Breaking problems into smaller parts and using algorithms, logic, and abstraction to develop solutions. Often used but not limited to computer science.

**Congestive Heart Failure:** A chronic condition in which the heart cannot pump enough blood to meet the body's needs, resulting in fluid buildup and symptoms such as shortness of breath and fatigue.

**CRISP-DM:** Cross-Industry Standart Process for Data Mining is a widely used methodology for data mining and analytics projects encompassing six phaes: Business understanding, data understanding, data preparation, modeling, evaluation, and deployment.

**C++:** A general purpose programming language. It's an extension of the C programming language

**Data Algorithms:** Computational procedures and mathematical models used to process and analyze data made accessible in the cloud for data scientists to deploy on large datasets efficiently.

**Data Analysis:** The process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making.

**Data at Rest:** Data that is stored and not actively in motion, typically residing in a database or storage system for various purposes, including backup.

**Data Cleansing:** The process of identifying and correcting or removing errors, inconsistencies in a dataset to improve its quality and reliability.

**Data Clusters:** A group of similar, related data points distinct from other clusters.

**Data Collection:** The process of gathering data from various sources, including demographics, clinical, coverage, and pharmaceutical information.

**Data Compilation:** The process of organizing and structuring data to create a comprehensive dataset. 

**Data File Types:** A computer file configuration is desired to store data in a specific way.

**Data Format:** How data is encoded so it can be stored within a data file type.

**Data Formatting:** The process of standardizing the data to ensure uniformity and ease of analysis.

**Data Integration:** A discipline involving practices, architectural techniques, and tools that enable organizations to ingest, transform, combine, and provision data across various data types, used for purposes such as data consistency, master data management, data sharing, and data migration.

**Data Lake:** A data repository for storing large volumes of structured, semi-structured, and unstructured data in its native format, faciliating agile data exploration and analysis.

**Data Manipulation:** The process of transforming data into a usable format.

**Data Mart:** A subset of a data warehose designed for specific business functions or user communities, providing isolated security and performance for focused analytics.

**Data Mining:** The process of automatically searching and analyzing data to discover patterns and insights that were previously unknown.

**Data Pipeline:** A comprehensive data movement process that covers the entire journey of data from source systems to destination systems, which includes data integration as a key component.

**Data Preparation:** The process of organizing and formatting data to meet the requirements of the modeling technique. 

**Data Quality:** Assessment of data integrity and completeness, addressing missing, invalid, or misleading values.

**Data Quality Assessment:** The evaluation of data integrity, accuracy, and completeness.

**Data Replication:** A strategy in which data is duplicated across multiple nodes in a cluster to ensure data durability and availability, reducing the risk of data loss due to hardware failures.

**Data Repository:** A general term referring to data that has been collected, organized, and isolated for business operations or data analysis. It can include databases, data warehouses, and big data stores.

**Data Requirements:** The identification and definition of the necessary data elements, formats, and sources required for analysis.

**Data Science:** An interdisciplinary field that involves extracting insights and knowledge from data using various techniques, including programming, statistics, and analytical tools.

**Data Science Methodology:** A structured approach to solving business problems using data analysis and data-driven insights.

**Data Scientist:** Professionals with data science and analytics expertise who apply their skills to solve business problems.

**Data Set:** A structured collection of data.

**Data Strategy:** A plan that outlines how an organization will collect, manage, and use data to achieve its goals.

**Data Understanding:** A stage where data scientists discuss various ways to manage data effectively, including automating certain processes in the database.

**Data Visualization:** A visual way, such as a graph, of representing data in a readily understandable way. Makes it easier to see trends in the data.

**Data Warehouse:** A central repository that consolidates data from various sources through the Extract, Transform, and Load (ETL) process, making it accessible for analytics and business intelligence.

**Data-Driven Insights:** Insights derived from analyzing and interpreting data to inform decision-making.

**DBAs (Database Administrators):** The professionals who are responsible for managing and extracting data from databases.

**Decision Tree:** A supervised machine learning algorithm that uses a tree-like structure of decisions and their possible consequences to make predictions or classify instances.

**Decision Tree Classification Model:** A model that uses a tree-like structure to classify data based on conditions and thresholds, provides predicted outcomes and associated probabilities.

**Decision Tree Classifier:** A classification model that uses a decision tree to determine outcomes based on specific conditions and thresholds.

**Decision Tree Model** A model used to review scenarios and identify relationships in data, such as the reasons for patient readmissions.

**Delimited text file formats:** Text files are used to store data where each line or row has values separated by a delimiter. A delimiter is a sequence of one or more characters specifying the boundary between values. Common delimiters include comma, tab, colon, vertical bar, and space.

**Deep Learning:** A subset of machine learning that involves artificial neural networks inspired by the human brain, capable of learning and making complex decisions from data on their own.

**Deep Learning4:** Language for deep learning.

**Deep Learning Models:** Includes Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) that create new data instances by learning patterns from large datasets.

**Delimited Text File:** A plain text file where a specific character separates the data values.

**Demographic Information:** Information about one's characteristics, such as age, gender, and location.

**Descriptive Approach:** An approach used to show relationships and identify clusters of similar activities based on events and preferences.

**Descriptive Modelling:** Modeling technique that focuses on describing and summarizing data, often through statistical analysis and visualization, without making predictions or inferences.

**Descriptive Statistics:** Techniques used to analyze and summarize data, providing initial insights and identify gaps in data.

**Digital Change:** The integration of digital technology into business processes and operations. Leads to improvements and innovations in how organizations operate and deliver value to customers.

**Digital Transformation:** A strategic and cultural organizational change driven by data science, especially by Big Data, to integrate digital technology across all areas of the organization, resulting in fundamental operational and value delivery changes.

**Distributed Data:** The practice of dividing data into smaller chunks and distributing them across multiple computers within a cluster. Enables parallel processing for data analysis.

**Document-based Database:** A type of NoSQL database that stores each record and its associated data within a single document, allowing flexible indexing, ad hoc queries, and analytics over collections of documents.

**Domain Knowledge:** Expertise and understanding of a specific subject area or field, including its concepts, principles, and relevant data.

**ETL Process:** The Extract, Transform, and Load process for data integration involves extracting data from various sources, transforming it into a usable format, and loading it into a repository.

**Executive Summary:** Usually occurring at the beginning of a research paper, this section summarizes the important parts of the paper, including its key findings.

**Extensible Markup Language (XML):** A language designed to structure, store, and enable data exchange between various technologies.

**Feature:** A characteristic or attribute within the data that helps in solving the problem.

**Feature Engineering:** The process of creating new features or variables based on domain knowledge to improve ML algorithm's performance.

**Feature Extraction:** Identifying and selecting relevant features or attributes from the data set.

**Five V's of Big Data:** Characteristics used to describe big data: Velocity, Volume, Variety, Veracity, and Volume.

**FSF:** Free Software Foundation

**Generative AI:** A subset of AI that focuses on new data, such as images, music, text, or code, rather than just analyzing existing data.

**Git:** De facto standard for code asset management, also know as version management or version control. Around Git emerged several services, GitHub, and GitLab.

**ggplot2:** A popular library for data visualization in R.

**Goals and Objectives:** The sought-after outcomes and specific objectives that support the overall goal of the task or problem.

**GPU:** Graphic processing units.

**Graph-based Database:** A type of NoSQL database that uses a graphical model to represent and store data, ideal for visualizing, analyzing, and discovering connections between interconnected data points.

**Hadoop:** A distributed storage and processing framework used for handling and analyzing large datasets, particularly well-suited for big data analytics and data science applications.

**Hadoop Distributed File System (HDFS):** A storage system within the Hadoop framework that partitions and distributes files across multiple nodes, faciliating parallel data access and fault tolerance.

**High Performing Computing (HPC) Cluster:** A computing technology that uses a system of networked computers designed to solve complex and computationally intensive problems in traditional environments.

**Infrastructure as a Service (IaaS):** A could service model that provides access to computing infrastructure, including servers, storage, and networking, without the need for users to manage or operate them.

**Interactive Processes:** Iterative and continuous refinement of the methodology based on insights and feedback from data analysis.

**Intermediate Results:** Partial results obtained from predictive modeling, can influence decisions on acquiring additional data.

**Iteration:** A single cycle or repetition of a process, often involves refining or modifiying a solution based on feedback or new information.

**Iterative Process:** A process that involves repeating a series of steps or actions to refine and improve a solution or analysis. Each iteration builds upon the previous one.

**Java:** An open-source, object oriented high-level programming language.

**Java-Based Framework:** Hadoop is implemented in Java, an open-source, high-level programming language, providing the foundation for building distributed storage and processing solutions.

**Java-ML:** Language for machine learning.

**JavaScript:** A general-purpose language that extended beyond the browser with the creation of Node.js and other server-side approaches.

**JavaScript Object Notation:** A data format compatible with various programming languages for two applications to exchange structured data.

**Julia:** A language for high-performance numerical analysis and computational science.

**Jupyter Notebooks:** A computational environment that allows users to create and share documents containing code, equations, visualizations, and explanatory text. See "Python notebooks".

**JupyterLab:** A browser-based application that allows you to access multiple Jupyter Notebook files, other code, and data files.

**Kernel:** An execution environment for different programming languages.

**Key-Value Store:** A type of NoSQL database where data is stored as key-value pairs, with the key serving as a unique identifier and the value containing data, which can be simple or complex.

**Lattice:** A higher-level data visualization library that can handle graphics without customizations.

**Leaf:** The final nodes of a decision tree where data is categorized into specific outcomes.

**Leaflet:** Used for creating interactive plots.

**Library:** A collection of functions and methods that allow you to perform many actions without writing the code.

**Map Process:** The initial step in Hadoop's MapReduce programming model, where data is processed in parallel on individual cluster nodes, often used for data transformation tasks.

**Market Basket Analysis:** Analyzing which goods tend to be bought together. Is often used for marketing insights.

**Mathematical Computing:** The use of computers to calculate, simulate, and model mathematical problems.

**Matplotlib:** A package for data visualization.

**Matrices:** Plural for matric, matrices are a rectangular (tabular) array of numbers often used in mathematics, statistics, and computer science.

**Mean:** The average value of a set of numbers. Is calculated by summing all the values and dividing by the total number of values.

**Median**: When arranged in ascending or descending order, the middle value in a set of numbers. Divies the data into two equal halves.

**Missing Values:** Values that are absent or unknown in the dataset, requiring careful handling during the data preparation.

**Model:** A representation of the relationships and patterns found in data to make predictions or analyze complex systems retaining essential elements needed for analysis.

**Model Building:** The process of developing predictive models to gain insights and make informed decisions based on data analysis.

**Model Calibration:** Adjusting model parameters to improve accuracy and alignment with the initial design.

**Model Training:** The process by which the model learns patterns from data.

**MongoDB:** A NoSQL database for big data management that was built with C++

**Naive Bayes:** A simple probabilistic classification algorithm based on Bayes' theorem.

**Natural Language Processing (NLP):** A field of AI that enables machines to understand, generate, and interact with human language, revolutionizing content creation and chat bots.

**Nearest Neighbor:** A machine learning algorithm that predicts a target variable based on its similarity to other values in the dataset.

**Neural Networks:** A computational model used in deep learning that mimics the structure and functioning of the human brain's neural pathways. It takes an input, processes it using previous learning, and produces an output.

**NoSQL Databases:** Databases designed to store and manage unstructured data and provide analysis tools for examining this type of data.

**NumPy:** Libraries based on arrays and matrices, allowing you to apply mathematical functions to the arrays.

**Online Transaction Processing (OLTP) Systems:** Systems that focus on handling business transactions and storing structured data.

**Outliers:** When a data point or points occur significantly outside of most of the other data in a data set, potentially indicating anomalies, errors, or unique phenomena that could impact statistical analysis or modelling.

**Pairwise Correlations:** An analysis to determine the relationships and correlations between variable types.

**Pandas:** An open-source Python library that provides tools for working with structured data. Is often used for data manipulation and analysis.

**Pairwise comparison (correlation):** A statistical technique that measures the strength and direction of the linear relationship between two variables by calculation a correlation coefficient.

**Pattern:** A recurring or noticeable arrangement or sequence in data. Can provide insights or be used for prediction or classification.

**Plotly:** Used for web-based data visualizations that can be displayed or saved as individual HTML files.

**Portability:** The capability of data integration tools to be used in various environments, including single-cloud, multi-cloud, or hybrid-cloud scenarios, providing flexibility in deployment options.

**Pre-built Connectors:** Cataloged connectors and adapters that simplify connecting and building integration flows with diverse data sources like databases, flat files, social media, APIs, CRM, and ERP applications.

**Precision vs Recall:** Metrics are used to evaluate the performance of classification models.

**Predictive Analysis:** Using data, algorithms, models, and machine learning to predict future outcomes or events.

**Predictive Model:** A model used to determine probabilities of an action or outcome based on historical data.

**Predictors:** Variables or features in a model that are used to predict or explain the outcome variable or target variable.

**Prioritization:** The process of organizing objectives and tasks based on their importance and impact on the overall goal.

**Problem Solving:** The process of addressing challenges and finding solutions to achieve desired outcomes.

**Python:** A high-level, general purpose programming language. It has a large, standard library that provides tools suited to many different tasks, including databases, automation, web scraping, text processing, image processing, machine learning, and data analytics. 

**Python Notebooks:** Also known as Jupyter notebook, this computational environment allows users to create and share documents containing code, equations, visualizations, and explanatory text.

**Quantitative Analysis:** A systematic approach using mathematical and statistical analysis is used to interpret numerical data.

**R:** An open-source programming language used for statistical computing, data analysis, and data visualization.

**Regression Models:** Are used to predict a numeric (or "real") value.

**Reinforcement Learning:** Loosely based on the way human beings and other organisms learn.

**Relational Databases:** Databases designed to store structured data with well-defined schemas and support standard data analysis methods and tools.

**REST:** Representational State Transfer.

**Rstudio:** Unifies programming, execution, debugging, remote data access, data exploration, and visualization into one tool.

**Scala:** Is a combination of the words "scalable" and "language". A general-purpose programming language that provides support for functional programming and is a strong static type system.

**Scalability:** The ability of a data repository to grow and expand its capacity to handle increasing data volumes and workload demands over time.

**Schema:** The predefined structure that describes the organization and format of data within a database, indicating the types of data allowed and their relationships.

**Sensors:** Devices such as Global Positioning Systems (GPS) and Radio Frequency Identification (RFID) tags generate structured data.

**Spreadsheets:** Software applications like Excel, used for organizing and analyzing structured data.

**Spyder:** Integrates code, documentation, and visualizations, among others, into a single canvas.

**SQL:** Structured Query Language that is non-procedural, used for querying and managing data.

**SQL Databases:** Databases that use Structured Query Language (SQL) for defining, manipulating, and querying data in structured formats.

**Stakeholders:** Individuals or groups with a vested interest in the data science model's outcome and its practical application, such as solution owners, marketing, application developers, and IT administration.

**Standard Deviation:** A measure of the dispersion or variability of a set of values from their mean. It provides information about the spread or distribution of the data.

**Stata:** A software package used for statistical analysis.

**Statistical Analysis:** Collection, organization, analysis, interpretation, and presentation of data to uncover patterns and trends.

**Statistical Distributions:** A way of describing the likelihood of different outcomes based on a dataset. The "bell curve" is a common statistical distribution.

**Statistics:** Collection, organization, analysis, interpretation, and presentation of data to uncover patterns and trends.

**Streaming Data:** Data that is continuously generated and transmitted in real-time. Requires specialized handling and processing to capture and analyze.

**Structured Data:** Data is organized and formatted into a predictable schema, usually related tables with rows and columns.

**Structured Data (Data Model):** Data organized and formatted according to a predefined schema or model and is typically stored in a databases or spreadsheets.

**Structured Query Language (SQL):** A language used for managing data in a relational database.

**Supervised Learning:** A learning in which a human provides input data and correct outputs.

**Synthetic Data:** Artificially generated data with properties similar to real data, used by data scientists to augment their datasets and improve model training.

**Tab-separated Values (TSVs):** Delimited text files where the delimiter is a tab. Used as an alternative to CSV.

**TCP/IP Network:** A network that uses the TCP/IP protocol to communicate between connected devices on that network. The Intertet uses TCP/IP.

**TensorFlow:** Deep learning library for dataflow that was built with C++

**Text Analysis:** Steps to analyze and manipulate textual data, extracting meaningful information and patterns.

**Text Analysis Data Mining:** The process of extracting useful information or knowledge from unstructured textual data through techniques such as natural language processing, text mining, and sentiment analysis.

**Text Analysis Groupings:** Creating meaningful groupings and categories from textual data for analysis.

**Threshold Value:** The specific value used to split data into groups or categories in a decision tree.

**Training Set:** A subset of data used to train or fit a machine learning model; consists of input data and corresponding known or labeled output values.

**Unavailable Data:** Data elements that are not currently accessible or integrated into the data sources.

**Univariate:** Modeling analysis focused on a single variable or feature at a time, considering its characteristics and relationship to other variables independently.

**Unstructured Data:** Unorganized data that lacks a predefined data model or organization makes it harder to analyze using traditional methods. This data type often includes text, images, videos, and other content that doesn't fit neatly into rows and columns like structured data.

**Unsupervised Learning:** The data is not labeled by a human. Examples are Clustering models used to divide each record of a dataset into one of a similar group.

**Vendor Lock-in:** A situation where a user becomes dependent on a specific vendor's technologies and solutions, making it challenging to switch to other platforms.

**Visualization:** The process of representing data visually to gain insights into its content and quality.

**Visualization Techniques:** Methods and tools that data scientists use to create visual representations or graphics that enhance the accessibility and understanding of data patterns, relationships, and insights.

**Watson Studio:** A fully integrated development environment for data scientists.

**Weka:** Language for data mining.
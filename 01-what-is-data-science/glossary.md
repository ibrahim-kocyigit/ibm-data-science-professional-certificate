## Glossary for Course 1: What is Data Science

**ACID-compliance**  
Ensuring data accuracy and consistency through Atomicity, Consistency, Isolation, and Durability in database transactions.

**Adobe Spark**  
A suite of software tools that allow users to create and share visual content such as graphics, web pages, and videos.

**Algorithm**  
A set of step-by-step instructions to solve a problem or complete a task.

**Analytical Skills**  
The ability to analyze information systematically, logically, and organized.

**Analytics**  
The process of examining data to draw conclusions and make informed decisions. It's a fundamental aspect of data science, involving statistical analysis and data-driven insights.

**Arithmetic Models**  
Data science often uses mathematical models to analyze data and predict outcomes.

**Artificial Neural Networks**  
Collections of small computing units (neurons) that process data and learn to make decisions over time.

**Bayesian Analysis**  
A statistical technique that uses Bayes' theorem to update probabilities based on new evidence.

**Big Data**  
Vast amounts of structured, semi-structured, and unstructured data. Characterized by its volume, velocity, variety, and value, which, when analyzed, can provide competitive advantages and drive digital transformations.

**Big Data Cluster**  
A distributed computing environment comprising thousands or tens of thousands of interconnected computers that collectively store and process large datasets.

**Broad Network Access**  
The ability to access cloud resources via standard mechanisms and platforms such as mobile devices, laptops, and workstations over networks.

**Business Insights**  
Accurate insights and reports generated by generative AI. Can be updated as data evolves, enhancing decision-making and uncovering hidden patterns.

**Case study**  
In-depth analysis of an instance of a chosen subject to draw insights that inform theory, practice or decision-making.

**Chief Data Officer (CDO)**  
An emerging role responsible for overseeing data-related initiatives, governance, and strategies, ensuring that data plays a central role in digital transformation efforts.

**Chief Information Officer (CIO)**  
An executive who is responsible for managing an organization's information technologh and computer systems, contributing to technology-related aspects of digital transformation.

**Cloud-based Integration Platform as a Service (iPaaS)**  
Cloud-hosted integration platforms that offer integration services through virtual private clouds or hybrid cloud models, providing scalability and flexibility.

**Cloud Computing**  
The delivery of on-demand computing resources, including networks, servers, storage, applications, services, and data centers, over the internet on a pay-for-use basis.

**Cloud Deployment Models**  
Categories that indicate where cloud infrastructure resides, who manages it, and how cloud resources and services are made available to users, including public, private, and hybrid models.

**Cloud Service Models**  
Models based on the layers of a computing stack, including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS), represent different cloud computing offerings.

**Cluster Analysis**  
The process of grouping similar data points together based on certain futures or attributes.

**Coding Automation**  
Using generative AI to automatically generate and test software code for constructing analytical models, freeing data scientists to focus on higher-level tasks.

**Column-based Database**  
A type of NoSQL database that organizes data in cells grouped as columns, often used for systems requiring high write request volume and storage of time-series or IoT data.

**Comma-Separated Values (CSV) / Tab-Separated Values (TSv)**  
Commonly used format for storing tabular data as plain text where either the comma or the tab separates each value.

**Commodity Hardware**  
Standard, off-the-shelf hardware components, are used in a big data cluster, offering cost-effective solutions for storage and processing without relying on specialized hardware.

**Computational Thinking**  
Breaking problems into smaller parts and using algorithms, logic, and abstraction to develop solutions. Often used but not limited to computer science.

**Data Algorithms**  
Computational procedures and mathematical models used to process and analyze data made accessible in the cloud for data scientists to deploy on large datasets efficiently.

**Data at Rest**  
Data that is stored and not actively in motion, typically residing in a database or storage system for various purposes, including backup.

**Data Clusters**  
A group of similar, related data points distinct from other clusters.

**Data File Types**  
A computer file configuration is desired to store data in a specific way.

**Data Format**  
How data is encoded so it can be stored within a data file type.

**Data Integration**  
A discipline involving practices, architectural techniques, and tools that enable organizations to ingest, transform, combine, and provision data across various data types, used for purposes such as data consistency, master data management, data sharing, and data migration.

**Data Lake**  
A data repository for storing large volumes of structured, semi-structured, and unstructured data in its native format, faciliating agile data exploration and analysis.

**Data Mart**  
A subset of a data warehose designed for specific business functions or user communities, providing isolated security and performance for focused analytics.

**Data Mining**  
The process of automatically searching and analyzing data to discover patterns and insights that were previously unknown.

**Data Pipeline**  
A comprehensive data movement process that covers the entire journey of data from source systems to destination systems, which includes data integration as a key component.

**Data Replication**  
A strategy in which data is duplicated across multiple nodes in a cluster to ensure data durability and availability, reducing the risk of data loss due to hardware failures.

**Data repository**  
A general term referring to data that has been collected, organized, and isolated for business operations or data analysis. It can include databases, data warehouses, and big data stores.

**Data Science**  
An interdisciplinary field that involves extracting insights and knowledge from data using various techniques, including programming, statistics, and analytical tools.

**Data Strategy**  
A plan that outlines how an organization will collect, manage, and use data to achieve its goals.

**Data Visualization**  
A visual way, such as a graph, of representing data in a readily understandable way. Makes it easier to see trends in the data.

**Data Warehouse**  
A central repository that consolidates data from various sources through the Extract, Transform, and Load (ETL) process, making it accessible for analytics and business intelligence.

**Decision Trees**  
A type of machine learning algorithm used for decision-making by creating a tree-like structure of decisions.

**Delimited text file formats**  
Text files are used to store data where each line or row has values separated by a delimiter. A delimiter is a sequence of one or more characters specifying the boundary between values. Common delimiters include comma, tab, colon, vertical bar, and space.

**Deep Learning**  
A subset of machine learning that involves artificial neural networks inspired by the human brain, capable of learning and making complex decisions from data on their own.

**Deep Learning Models**  
Includes Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) that create new data instances by learning patterns from large datasets.

**Delimited Text File**  
A plain text file where a specific character separates the data values.

**Digital Change**  
The integration of digital technology into business processes and operations. Leads to improvements and innovations in how organizations operate and deliver value to customers.

**Digital Transformation**  
A strategic and cultural organizational change driven by data science, especially by Big Data, to integrate digital technology across all areas of the organization, resulting in fundamental operational and value delivery changes.

**Distributed Data**  
The practice of dividing data into smaller chunks and distributing them across multiple computers within a cluster. Enables parallel processing for data analysis.

**Document-based Database**  
A type of NoSQL database that stores each record and its associated data within a single document, allowing flexible indexing, ad hoc queries, and analytics over collections of documents.

**ETL Process**  
The Extract, Transform, and Load process for data integration involves extracting data from various sources, transforming it into a usable format, and loading it into a repository.

**Executive Summary**  
Usually occurring at the beginning of a research paper, this section summarizes the important parts of the paper, including its key findings.

**Extensible Markup Language (XML)**  
A language designed to structure, store, and enable data exchange between various technologies.

**Five V's of Big Data**  
Characteristics used to describe big data: Velocity, Volume, Variety, Veracity, and Volume.

**Generative AI**  
A subset of AI that focuses on new data, such as images, music, text, or code, rather than just analyzing existing data.

**Graph-based Database**  
A type of NoSQL database that uses a graphical model to represent and store data, ideal for visualizing, analyzing, and discovering connections between interconnected data points.

**Hadoop**  
A distributed storage and processing framework used for handling and analyzing large datasets, particularly well-suited for big data analytics and data science applications.

**Hadoop Distributed File System (HDFS)**  
A storage system within the Hadoop framework that partitions and distributes files across multiple nodes, faciliating parallel data access and fault tolerance.

**High Performing Computing (HPC) Cluster**  
A computing technology that uses a system of networked computers designed to solve complex and computationally intensive problems in traditional environments.

**Infrastructure as a Service (IaaS)**  
A could service model that provides access to computing infrastructure, including servers, storage, and networking, without the need for users to manage or operate them.

**Java-Based Framework**  
Hadoop is implemented in Java, an open-source, high-level programming language, providing the foundation for building distributed storage and processing solutions.

**JavaScript Object Notation**  
A data format compatible with various programming languages for two applications to exchange structured data.

**Jupyter Notebooks**  
A computational environment that allows users to create and share documents containing code, equations, visualizations, and explanatory text. See "Python notebooks".

**Key-Value Store**  
A type of NoSQL database where data is stored as key-value pairs, with the key serving as a unique identifier and the value containing data, which can be simple or complex.

**Map Process**  
The initial step in Hadoop's MapReduce programming model, where data is processed in parallel on individual cluster nodes, often used for data transformation tasks.

**Market Basket Analysis**  
Analyzing which goods tend to be bought together. Is often used for marketing insights.

**Mathematical Computing**  
The use of computers to calculate, simulate, and model mathematical problems.

**Matrices**  
Plural for matric, matrices are a rectangular (tabular) array of numbers often used in mathematics, statistics, and computer science.

**Model**  
A representation of the relationships and patterns found in data to make predictions or analyze complex systems retaining essential elements needed for analysis.

**Naive Bayes**  
A simple probabilistic classification algorithm based on Bayes' theorem.

**Natural Language Processing (NLP)**  
A field of AI that enables machines to understand, generate, and interact with human language, revolutionizing content creation and chatbots.

**Nearest Neighbor**  
A machine learning algorithm that predicts a target variable based on its similarity to other values in the dataset.

**Neural Networks**  
A computational model used in deep learning that mimics the structure and functioning of the human brain's neural pathways. It takes an input, processes it using previous learning, and produces an output.

**NoSQL Databases**  
Databases designed to store and manage unstructured data and provide analysis tools for examining this type of data.

**Online Transaction Processing (OLTP) Systems**  
Systems that focus on handling business transactions and storing structured data.

**Outliers**  
When a data point or points occur significantly outside of most of the other data in a data set, potentially indicating anomalies, errors, or unique phenomena that could impact statistical analysis or modelling.

**Pandas**  
An open-source Python library that provides tools for working with structured data. Is often used for data manipulation and analysis.

**Portability**  
The capability of data integration tools to be used in various environments, including single-cloud, multi-cloud, or hybrid-cloud scenarios, providing flexibility in deployment options.

**Pre-built Connectors**  
Cataloged connectors and adapters that simplify connecting and building integration flows with diverse data sources like databases, flat files, social media, APIs, CRM, and ERP applications.

**Precision vs Recall**  
Metrics are used to evaluate the performance of classification models.

**Predictive Analysis**  
Using data, algorithms, models, and machine learning to predict future outcomes or events.

**Python Notebooks**  
Also known as Jupyter notebook, this computational environment allows users to create and share documents containing code, equations, visualizations, and explanatory text.

**Quantitative Analysis**  
A systematic approach using mathematical and statistical analysis is used to interpret numerical data.

**R**  
An open-source programming language used for statistical computing, data analysis, and data visualization.

**Relational Databases**  
Databases designed to store structured data with well-defined schemas and support standard data analysis methods and tools.

**Scalability**  
The ability of a data repository to grow and expand its capacity to handle increasing data volumes and workload demands over time.

**Schema**  
The predefined structure that describes the organization and format of data within a database, indicating the types of data allowed and their relationships.

**Sensors**  
Devices such as Global Positioning Systems (GPS) and Radio Frequency Identification (RFID) tags generate structured data.

**Spreadsheets**  
Software applications like Excel, used for organizing and analyzing structured data.

**SQL Databases**  
Databases that use Structured Query Language (SQL) for defining, manipulating, and querying data in structured formats.

**Stata**  
A software package used for statistical analysis.

**Statistical Distributions**  
A way of describing the likelihood of different outcomes based on a dataset. The "bell curve" is a common statistical distribution.

**Streaming Data**  
Data that is continuously generated and transmitted in real-time. Requires specialized handling and processing to capture and analyze.

**Structured Data**  
Data is organized and formatted into a predictable schema, usually related tables with rows and columns.

**Structured Query Language (SQL)**  
A language used for managing data in a relational database.

**Synthetic Data**  
Artificially generated data with properties similar to real data, used by data scientists to augment their datasets and improve model training.

**Tab-separated Values (TSVs)**  
Delimited text files where the delimiter is a tab. Used as an alternative to CSV.

**TCP/IP Network**  
A network that uses the TCP/IP protocol to communicate between connected devices on that network. The Intertet uses TCP/IP.

**Unstructured Data**  
Unorganized data that lacks a predefined data model or organization makes it harder to analyze using traditional methods. This data type often includes text, images, videos, and other content that doesn't fit neatly into rows and columns like structured data.

**Vendor Lock-in**  
A situation where a user becomes dependent on a specific vendor's technologies and solutions, making it challengin to switch to other platforms.
